{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import logging\n",
    "import gensim\n",
    "import nltk.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vdelavaud\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\vdelavaud\\\\Documents'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "#os.chdir(\"Documents\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Le Text Mining est un ensemble de methodes  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Pour extraire du sens de documents non struct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Le Text Mining est (quasi) aussi ancien que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>La mesure statistique de la frequence des mot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>Pour la petite histoire  l'IBM 704 sorti en 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0    Le Text Mining est un ensemble de methodes  ...\n",
       "1   2      0   Pour extraire du sens de documents non struct...\n",
       "2   3      0     Le Text Mining est (quasi) aussi ancien que...\n",
       "3   5      0   La mesure statistique de la frequence des mot...\n",
       "4   6      0   Pour la petite histoire  l'IBM 704 sorti en 1..."
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lecture du fichier \n",
    "train = pd.read_csv('classeur1.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faire ressortir les particularités du fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pour afficher : train[['tweet','nomMethode']].head()\n",
    "\n",
    "# Nombre de mots pour chaque tweet\n",
    "train['word_count'] = train['tweet'].apply(lambda x: len(str(x).split(\" \")))\n",
    "\n",
    "# Nombre de caractères pour chaque tweet\n",
    "train['char_count'] = train['tweet'].str.len() ## this also includes spaces\n",
    "\n",
    "# Taille moyenne des mots\n",
    "def avg_word(sentence):\n",
    "  words = sentence.split()\n",
    "  return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "train['avg_word'] = train['tweet'].apply(lambda x: avg_word(x))\n",
    "\n",
    "# Nombre de stopwords\n",
    "stop = stopwords.words('french')\n",
    "train['stopwords'] = train['tweet'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "\n",
    "# Nombre de caractères spéciaux // Pour le cas du Tweet on prends en comtpe le #\n",
    "#train['hastags'] = train['tweet'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "\n",
    "# Nombre de numérique\n",
    "train['numerics'] = train['tweet'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "\n",
    "# Nombre de lettre majuscule (Symbole de colère)\n",
    "train['upper'] = train['tweet'].apply(lambda x: len([x for x in x.split() if x.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>numerics</th>\n",
       "      <th>upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Le Text Mining est un ensemble de methodes  ...</td>\n",
       "      <td>47</td>\n",
       "      <td>259</td>\n",
       "      <td>5.071429</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Pour extraire du sens de documents non struct...</td>\n",
       "      <td>62</td>\n",
       "      <td>392</td>\n",
       "      <td>5.910714</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Le Text Mining est (quasi) aussi ancien que...</td>\n",
       "      <td>12</td>\n",
       "      <td>63</td>\n",
       "      <td>5.777778</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>La mesure statistique de la frequence des mot...</td>\n",
       "      <td>42</td>\n",
       "      <td>263</td>\n",
       "      <td>5.414634</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>Pour la petite histoire  l'IBM 704 sorti en 1...</td>\n",
       "      <td>46</td>\n",
       "      <td>260</td>\n",
       "      <td>4.886364</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  word_count  \\\n",
       "0   1      0    Le Text Mining est un ensemble de methodes  ...          47   \n",
       "1   2      0   Pour extraire du sens de documents non struct...          62   \n",
       "2   3      0     Le Text Mining est (quasi) aussi ancien que...          12   \n",
       "3   5      0   La mesure statistique de la frequence des mot...          42   \n",
       "4   6      0   Pour la petite histoire  l'IBM 704 sorti en 1...          46   \n",
       "\n",
       "   char_count  avg_word  stopwords  numerics  upper  \n",
       "0         259  5.071429         11         0      0  \n",
       "1         392  5.910714         16         0      0  \n",
       "2          63  5.777778          2         0      0  \n",
       "3         263  5.414634         17         0      0  \n",
       "4         260  4.886364         11         3      1  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nettoyage du fichier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 1 : Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pour afficher un aperçu, mettre train['tweet'].head()\n",
    "\n",
    "# Tout mettre en minuscule\n",
    "train['tweet'] = train['tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "\n",
    "# Supprimer la ponctuation\n",
    "train['tweet'] = train['tweet'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# Supprimer les stopwords\n",
    "train['tweet'] = train['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "# Recuperer les mots qui reviennent trop fréquemment, les afficher\n",
    "freq = pd.Series(' '.join(train['tweet']).split()).value_counts()[:5]\n",
    "# Les supprimer\n",
    "freq = list(freq.index)\n",
    "train['tweet'] = train['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "\n",
    "# Supprimer les mots rares (Ils sont trop rares pour leur donner du sens, on va donc les supprimer pour obtenir une meilleur compréhension)\n",
    "freq = pd.Series(' '.join(train['tweet']).split()).value_counts()[-5:]\n",
    "freq = list(freq.index)\n",
    "train['tweet'] = train['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "\n",
    "# Correcteur d'orthographe\n",
    "train['tweet'][:5].apply(lambda x: str(TextBlob(x).correct()))\n",
    "\n",
    "# Tokenization\n",
    "TextBlob(train['tweet'][1]).words\n",
    "\n",
    "# Stemming (Racinisation)\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "st = FrenchStemmer()\n",
    "train['tweet'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "\n",
    "# Lemmatization\n",
    "from textblob import Word\n",
    "train['tweet'] = train['tweet'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ensemble methodes technique doutils exploiter ...\n",
       "1    extraire sappuie technique danalyse linguistiq...\n",
       "2                     quasi aussi ancien linformatique\n",
       "3    mesure frequence distribution permet detablir ...\n",
       "4    petite histoire libm sorti 1955 a epoque cest ...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 2 : Complexe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\vdelavaud\\\\Documents'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.091342  ,  0.223404  ,  0.58856   , -0.614765  , -0.0838365 ,\n",
       "        0.5387    , -0.43531   ,  0.349125  ,  0.163308  , -0.28223   ,\n",
       "        0.53547   ,  0.52797496,  0.096812  ,  0.2879    , -0.0533385 ,\n",
       "       -0.37232   ,  0.022637  ,  0.574705  , -0.553275  ,  0.385575  ,\n",
       "        0.565335  ,  0.805405  ,  0.2579965 ,  0.0088565 ,  0.1674905 ,\n",
       "        0.25543   , -0.571035  , -0.59926   ,  0.422585  , -0.42896   ,\n",
       "       -0.389065  ,  0.19631   , -0.00933   ,  0.127285  , -0.0487465 ,\n",
       "        0.381435  , -0.22540998,  0.021299  , -0.1827915 , -0.16490501,\n",
       "       -0.47944498, -0.431528  , -0.20091   , -0.55665   , -0.32982   ,\n",
       "       -0.088548  , -0.28038502,  0.219725  ,  0.090537  , -0.67012   ,\n",
       "        0.0883085 , -0.19332   ,  0.0465725 ,  1.160815  ,  0.0691255 ,\n",
       "       -2.47895   , -0.33707   ,  0.083195  ,  1.86185   ,  0.283465  ,\n",
       "       -0.13081   ,  0.927795  , -0.37028   ,  0.1885465 ,  0.66198   ,\n",
       "        0.505175  ,  0.37748498,  0.1322995 , -0.380375  , -0.025135  ,\n",
       "       -0.1636765 , -0.45639   , -0.047815  , -0.87394   ,  0.0264145 ,\n",
       "        0.0117645 , -0.427415  , -0.31048   , -0.317725  , -0.02326   ,\n",
       "        0.525635  ,  0.05760051, -0.69786   , -0.1213325 , -1.2707    ,\n",
       "       -0.225355  , -0.1018815 ,  0.18575001, -0.30943   , -0.211059  ,\n",
       "       -0.279545  , -0.16002001,  0.100371  , -0.05461   , -0.71834505,\n",
       "       -0.392925  ,  0.12075999,  0.61991   ,  0.582145  ,  0.20161   ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N-grams (Determiner le sens à partir que la combinaison de n mots)\n",
    "TextBlob(train['tweet'][0]).ngrams(2)\n",
    "# On peut aussi l'utiliser sur 3 mots\n",
    "# TextBlob(train['tweet'][0]).ngrams(3)\n",
    "\n",
    "\n",
    "# Term frequency (TF)\n",
    "# Ratio d'un mot en fonction du tweet\n",
    "# TF = nbr de fois où le terme T apparait dans une ligne / nbr de termes dans la ligne\n",
    "tf1 = (train['tweet'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
    "tf1.columns = ['words','tf']\n",
    "tf1\n",
    "\n",
    "# Inverse Document Frequency (IDF)\n",
    "# Un mot ne nous est pas très utile s'il apparaît dans tous les documents\n",
    "#IDF = log(N/n), where, N is the total number of rows and n is the number of rows in which the word was present.\n",
    "for i,word in enumerate(tf1['words']):\n",
    "  tf1.loc[i, 'idf'] = np.log(train.shape[0]/(len(train[train['tweet'].str.contains(word)])))\n",
    "\n",
    "tf1\n",
    "\n",
    "# Frequence des termes TF-IDF (Term Frequency -Inverse Doc Freq)\n",
    "# Calcul de la fréquence d'un terme \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word',\n",
    " stop_words= 'english',ngram_range=(1,1))\n",
    "train_vect = tfidf.fit_transform(train['tweet'])\n",
    "\n",
    "train_vect\n",
    "\n",
    "# Bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow = CountVectorizer(max_features=1000, lowercase=True, ngram_range=(1,1),analyzer = \"word\")\n",
    "train_bow = bow.fit_transform(train['tweet'])\n",
    "train_bow\n",
    "    \n",
    "# Sentimental Analysis\n",
    "train['tweet'][:5].apply(lambda x: TextBlob(x).sentiment)\n",
    "train['sentiment'] = train['tweet'].apply(lambda x: TextBlob(x).sentiment[0] )\n",
    "train[['tweet','sentiment']].head()\n",
    "\n",
    "# Mot (qui vectorise d'autres mots)\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = 'glove.6B.100d.txt'\n",
    "word2vec_output_file = 'glove.6B.100d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "\n",
    "\n",
    "from gensim.models import KeyedVectors # load the Stanford GloVe model\n",
    "filename = 'glove.6B.100d.txt.word2vec'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "\n",
    "model['go']\n",
    "\n",
    "model['away']\n",
    "\n",
    "(model['go'] + model['away'])/2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons converti la chaîne entière en un vecteur qui peut maintenant être utilisé comme caractéristique dans n'importe quelle technique de modélisation.\n",
    "\n",
    "Il faut maintenant créer le modèle pour analyser les mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source\n",
    "# https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
