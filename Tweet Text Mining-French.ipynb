{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Charger les librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv                  # Permet la lecture des fichiers csv\n",
    "import pandas as pd         # Permet la manipulation des donneés\n",
    "import nltk                 # Natural Language Toolkit - Permet de travailler sur le langage humain\n",
    "import numpy as np          # Permet d'effectuer des opérations sur les données\n",
    "import logging              # Permet d'écrire simplement des logs en Python\n",
    "import gensim               # Permet le traitement de langage\n",
    "import nltk.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour utiliser textblob, il faut installer le pip :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ pip install -U textblob\n",
    "\n",
    "$ python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vdelavaud\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Charger l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\vdelavaud'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "#os.chdir(\"Documents\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  3. Lecture du fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Le Text Mining est un ensemble de methodes  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Pour extraire du sens de documents non struct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Le Text Mining est (quasi) aussi ancien que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>La mesure statistique de la frequence des mot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>Pour la petite histoire  l'IBM 704 sorti en 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0    Le Text Mining est un ensemble de methodes  ...\n",
       "1   2      0   Pour extraire du sens de documents non struct...\n",
       "2   3      0     Le Text Mining est (quasi) aussi ancien que...\n",
       "3   5      0   La mesure statistique de la frequence des mot...\n",
       "4   6      0   Pour la petite histoire  l'IBM 704 sorti en 1..."
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lecture du fichier \n",
    "train = pd.read_csv('classeur1.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faire ressortir les particularités du fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pour afficher : train[['tweet','nomMethode']].head()\n",
    "\n",
    "# Nombre de mots pour chaque tweet\n",
    "train['word_count'] = train['tweet'].apply(lambda x: len(str(x).split(\" \")))\n",
    "\n",
    "# Nombre de caractères pour chaque tweet\n",
    "train['char_count'] = train['tweet'].str.len() ## this also includes spaces\n",
    "\n",
    "# Taille moyenne des mots\n",
    "def avg_word(sentence):\n",
    "  words = sentence.split()\n",
    "  return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "train['avg_word'] = train['tweet'].apply(lambda x: avg_word(x))\n",
    "\n",
    "# Nombre de stopwords\n",
    "stop = stopwords.words('french')\n",
    "train['stopwords'] = train['tweet'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "\n",
    "# Nombre de caractères spéciaux // Pour le cas du Tweet on prends en comtpe le #\n",
    "#train['hastags'] = train['tweet'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "\n",
    "# Nombre de numérique\n",
    "train['numerics'] = train['tweet'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "\n",
    "# Nombre de lettre majuscule (Symbole de colère)\n",
    "train['upper'] = train['tweet'].apply(lambda x: len([x for x in x.split() if x.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>numerics</th>\n",
       "      <th>upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Le Text Mining est un ensemble de methodes  ...</td>\n",
       "      <td>47</td>\n",
       "      <td>259</td>\n",
       "      <td>5.071429</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Pour extraire du sens de documents non struct...</td>\n",
       "      <td>62</td>\n",
       "      <td>392</td>\n",
       "      <td>5.910714</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Le Text Mining est (quasi) aussi ancien que...</td>\n",
       "      <td>12</td>\n",
       "      <td>63</td>\n",
       "      <td>5.777778</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>La mesure statistique de la frequence des mot...</td>\n",
       "      <td>42</td>\n",
       "      <td>263</td>\n",
       "      <td>5.414634</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>Pour la petite histoire  l'IBM 704 sorti en 1...</td>\n",
       "      <td>46</td>\n",
       "      <td>260</td>\n",
       "      <td>4.886364</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  word_count  \\\n",
       "0   1      0    Le Text Mining est un ensemble de methodes  ...          47   \n",
       "1   2      0   Pour extraire du sens de documents non struct...          62   \n",
       "2   3      0     Le Text Mining est (quasi) aussi ancien que...          12   \n",
       "3   5      0   La mesure statistique de la frequence des mot...          42   \n",
       "4   6      0   Pour la petite histoire  l'IBM 704 sorti en 1...          46   \n",
       "\n",
       "   char_count  avg_word  stopwords  numerics  upper  \n",
       "0         259  5.071429         11         0      0  \n",
       "1         392  5.910714         16         0      0  \n",
       "2          63  5.777778          2         0      0  \n",
       "3         263  5.414634         17         0      0  \n",
       "4         260  4.886364         11         3      1  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Nettoyage du fichier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 1 : Nettoyer mot à mot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pour afficher un aperçu, mettre train['tweet'].head()\n",
    "\n",
    "# Tout mettre en minuscule\n",
    "train['tweet'] = train['tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "\n",
    "# Supprimer la ponctuation\n",
    "train['tweet'] = train['tweet'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# Supprimer les stopwords\n",
    "train['tweet'] = train['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "# Recuperer les mots qui reviennent trop fréquemment, les afficher\n",
    "freq = pd.Series(' '.join(train['tweet']).split()).value_counts()[:5]\n",
    "# Les supprimer\n",
    "freq = list(freq.index)\n",
    "train['tweet'] = train['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "\n",
    "# Supprimer les mots rares (Ils sont trop rares pour leur donner du sens, on va donc les supprimer pour obtenir une meilleur compréhension)\n",
    "freq = pd.Series(' '.join(train['tweet']).split()).value_counts()[-5:]\n",
    "freq = list(freq.index)\n",
    "train['tweet'] = train['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "\n",
    "# Correcteur d'orthographe\n",
    "train['tweet'][:5].apply(lambda x: str(TextBlob(x).correct()))\n",
    "\n",
    "# Tokenization\n",
    "TextBlob(train['tweet'][1]).words\n",
    "\n",
    "# Stemming (Racinisation)\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "st = FrenchStemmer()\n",
    "train['tweet'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "\n",
    "# Lemmatization\n",
    "from textblob import Word\n",
    "train['tweet'] = train['tweet'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afficher un aperçu :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-cca2e7ef5f37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "train['tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 2 : Nettoyer le mot selon le groupe de mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.091342  ,  0.223404  ,  0.58856   , -0.614765  , -0.0838365 ,\n",
       "        0.5387    , -0.43531   ,  0.349125  ,  0.163308  , -0.28223   ,\n",
       "        0.53547   ,  0.52797496,  0.096812  ,  0.2879    , -0.0533385 ,\n",
       "       -0.37232   ,  0.022637  ,  0.574705  , -0.553275  ,  0.385575  ,\n",
       "        0.565335  ,  0.805405  ,  0.2579965 ,  0.0088565 ,  0.1674905 ,\n",
       "        0.25543   , -0.571035  , -0.59926   ,  0.422585  , -0.42896   ,\n",
       "       -0.389065  ,  0.19631   , -0.00933   ,  0.127285  , -0.0487465 ,\n",
       "        0.381435  , -0.22540998,  0.021299  , -0.1827915 , -0.16490501,\n",
       "       -0.47944498, -0.431528  , -0.20091   , -0.55665   , -0.32982   ,\n",
       "       -0.088548  , -0.28038502,  0.219725  ,  0.090537  , -0.67012   ,\n",
       "        0.0883085 , -0.19332   ,  0.0465725 ,  1.160815  ,  0.0691255 ,\n",
       "       -2.47895   , -0.33707   ,  0.083195  ,  1.86185   ,  0.283465  ,\n",
       "       -0.13081   ,  0.927795  , -0.37028   ,  0.1885465 ,  0.66198   ,\n",
       "        0.505175  ,  0.37748498,  0.1322995 , -0.380375  , -0.025135  ,\n",
       "       -0.1636765 , -0.45639   , -0.047815  , -0.87394   ,  0.0264145 ,\n",
       "        0.0117645 , -0.427415  , -0.31048   , -0.317725  , -0.02326   ,\n",
       "        0.525635  ,  0.05760051, -0.69786   , -0.1213325 , -1.2707    ,\n",
       "       -0.225355  , -0.1018815 ,  0.18575001, -0.30943   , -0.211059  ,\n",
       "       -0.279545  , -0.16002001,  0.100371  , -0.05461   , -0.71834505,\n",
       "       -0.392925  ,  0.12075999,  0.61991   ,  0.582145  ,  0.20161   ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N-grams (Determiner le sens à partir que la combinaison de n mots)\n",
    "TextBlob(train['tweet'][0]).ngrams(2)\n",
    "# On peut aussi l'utiliser sur 3 mots\n",
    "# TextBlob(train['tweet'][0]).ngrams(3)\n",
    "\n",
    "\n",
    "# Term frequency (TF)\n",
    "# Ratio d'un mot en fonction du tweet\n",
    "# TF = nbr de fois où le terme T apparait dans une ligne / nbr de termes dans la ligne\n",
    "tf1 = (train['tweet'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
    "tf1.columns = ['words','tf']\n",
    "tf1\n",
    "\n",
    "# Inverse Document Frequency (IDF)\n",
    "# Un mot ne nous est pas très utile s'il apparaît dans tous les documents\n",
    "#IDF = log(N/n), where, N is the total number of rows and n is the number of rows in which the word was present.\n",
    "for i,word in enumerate(tf1['words']):\n",
    "tf1.loc[i, 'idf'] = np.log(train.shape[0]/(len(train[train['tweet'].str.contains(word)])))\n",
    "\n",
    "tf1\n",
    "\n",
    "# Frequence des termes TF-IDF (Term Frequency -Inverse Doc Freq)\n",
    "# Calcul de la fréquence d'un terme \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word',\n",
    "stop_words= 'english',ngram_range=(1,1))\n",
    "train_vect = tfidf.fit_transform(train['tweet'])\n",
    "\n",
    "train_vect\n",
    "\n",
    "# Bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow = CountVectorizer(max_features=1000, lowercase=True, ngram_range=(1,1),analyzer = \"word\")\n",
    "train_bow = bow.fit_transform(train['tweet'])\n",
    "train_bow\n",
    "    \n",
    "# Sentimental Analysis\n",
    "train['tweet'][:5].apply(lambda x: TextBlob(x).sentiment)\n",
    "train['sentiment'] = train['tweet'].apply(lambda x: TextBlob(x).sentiment[0] )\n",
    "train[['tweet','sentiment']].head()\n",
    "\n",
    "# Mot (qui vectorise d'autres mots)\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = 'glove.6B.100d.txt'\n",
    "word2vec_output_file = 'glove.6B.100d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "\n",
    "\n",
    "from gensim.models import KeyedVectors # load the Stanford GloVe model\n",
    "filename = 'glove.6B.100d.txt.word2vec'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "\n",
    "model['go']\n",
    "\n",
    "model['away']\n",
    "\n",
    "(model['go'] + model['away'])/2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons converti la chaîne entière en un vecteur qui peut maintenant être utilisé comme caractéristique dans n'importe quelle technique de modélisation.\n",
    "\n",
    "Il faut maintenant créer le modèle pour analyser les mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source\n",
    "# https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
